{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "#%pip install -r ../../agent_requirements.txt\n",
        "#dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
        "# To disable autoreload; run %autoreload 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "################################################################################### \n",
        "# Agent Chain Creation\n",
        "#\n",
        "# This notebook shows an example of a RAG-based Agent with multiple retrievers.\n",
        "#\n",
        "# Parameters:\n",
        "# * uc_catalog (required)                     - Name of the Unity Catalog \n",
        "# * schema (required)                         - Name of the schema inside Unity Catalog \n",
        "# * vector_search_endpoint (required)         - Name of the vector search endpoint\n",
        "# * vector_search_index (required)            - Name of the vector search index\n",
        "# * model_serving_endpopint (required)        - Name of the model endpoint to serve\n",
        "# * agent_model_endpoint (required)           - Name and Identifier of the agent model endpoint\n",
        "# * experiment (required)                     - Name of the experiment to register the run under\n",
        "# * registered_model (required)               - Name of the model to register in mlflow\n",
        "# * max_words (required)                      - Maximum number of words to return in the response\n",
        "# * model_alias (required)                    - Alias to give to newly registered model\n",
        "# * bundle_root (required)                    - Root of the bundle\n",
        "#\n",
        "# Widgets:\n",
        "# * Unity Catalog: Text widget to input the name of the Unity Catalog\n",
        "# * Schema: Text widget to input the name of the database inside the Unity Catalog\n",
        "# * Vector Search endpoint: Text widget to input the name of the vector search endpoint\n",
        "# * Vector search index: Text widget to input the name of the vector search index\n",
        "# * Agent model endppoint: Text widget to input the name of the agent model endpoint\n",
        "# * Experiment: Text widget to input the name of the experiment to register the run under\n",
        "# * Registered model name: Text widget to input the name of the model to register in mlflow\n",
        "# * Max words: Text widget to input the maximum integer number of words to return in the response\n",
        "# * Model Alias: Text widget to input the alias of the model to register in mlflow\n",
        "# * Bundle root: Text widget to input the root of the bundle\n",
        "#\n",
        "# Usage:\n",
        "# 1. Set the appropriate values for the widgets.\n",
        "# 2. Run the pipeline to create and register an agent with tool calling.\n",
        "#\n",
        "##################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of input args needed to run this notebook as a job\n",
        "# Provide them via DB widgets or notebook arguments\n",
        "\n",
        "# A Unity Catalog containing the preprocessed data\n",
        "dbutils.widgets.text(\n",
        "    \"uc_catalog\",\n",
        "    \"agentops_stacks_dev\",\n",
        "    label=\"Unity Catalog\",\n",
        ")\n",
        "# Name of schema\n",
        "dbutils.widgets.text(\n",
        "    \"schema\",\n",
        "    \"agentops\",\n",
        "    label=\"Schema\",\n",
        ")\n",
        "# Name of vector search endpoint containing the preprocessed index\n",
        "dbutils.widgets.text(\n",
        "    \"vector_search_endpoint\",\n",
        "    \"ai_agent_endpoint\",\n",
        "    label=\"Vector Search endpoint\",\n",
        ")\n",
        "# Name of vector search index containing the preprocessed index\n",
        "dbutils.widgets.text(\n",
        "    \"vector_search_index\",\n",
        "    \"databricks_documentation_vs_index\",\n",
        "    label=\"Vector Search index\",\n",
        ")\n",
        "# Foundational model to use\n",
        "dbutils.widgets.text(\n",
        "    \"agent_model_endpoint\",\n",
        "    \"databricks-meta-llama-3-3-70b-instruct\",\n",
        "    label=\"Agent model name\",\n",
        ")\n",
        "# Name of experiment to register under in mlflow\n",
        "dbutils.widgets.text(\n",
        "    \"experiment\",\n",
        "    \"agent_function_chatbot_dev\",\n",
        "    label=\"Experiment name\",\n",
        ")\n",
        "# Name of model to register in mlflow\n",
        "dbutils.widgets.text(\n",
        "    \"registered_model\",\n",
        "    \"agent_function_chatbot\",\n",
        "    label=\"Registered model name\",\n",
        ")\n",
        "# Max words for summarization\n",
        "dbutils.widgets.text(\n",
        "    \"max_words\",\n",
        "    \"20\",\n",
        "    label=\"Max Words\",\n",
        ")\n",
        "# Model alias\n",
        "dbutils.widgets.text(\n",
        "    \"model_alias\",\n",
        "    \"agent_latest\",\n",
        "    label=\"Model Alias\",\n",
        ")\n",
        "\n",
        "# Bundle root\n",
        "dbutils.widgets.text(\n",
        "    \"bundle_root\",\n",
        "    \"/\",\n",
        "    label=\"Root of bundle\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uc_catalog = dbutils.widgets.get(\"uc_catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "vector_search_endpoint = dbutils.widgets.get(\"vector_search_endpoint\")\n",
        "vector_search_index = dbutils.widgets.get(\"vector_search_index\")\n",
        "agent_model_endpoint = dbutils.widgets.get(\"agent_model_endpoint\")\n",
        "experiment = dbutils.widgets.get(\"experiment\")\n",
        "registered_model = dbutils.widgets.get(\"registered_model\")\n",
        "max_words = dbutils.widgets.get(\"max_words\")\n",
        "model_alias = dbutils.widgets.get(\"model_alias\")\n",
        "bundle_root = dbutils.widgets.get(\"bundle_root\")\n",
        "\n",
        "assert uc_catalog != \"\", \"uc_catalog notebook parameter must be specified\"\n",
        "assert schema != \"\", \"schema notebook parameter must be specified\"\n",
        "assert vector_search_endpoint != \"\", \"vector_search_endpoint notebook parameter must be specified\"\n",
        "assert vector_search_index != \"\", \"vector_search_index notebook parameter must be specified\"\n",
        "assert agent_model_endpoint != \"\", \"agent_model_endpoint notebook parameter must be specified\"\n",
        "assert experiment != \"\", \"experiment notebook parameter must be specified\"\n",
        "assert registered_model != \"\", \"registered_model notebook parameter must be specified\"\n",
        "assert max_words != \"\", \"max_words notebook parameter must be specified\"\n",
        "assert model_alias != \"\", \"model_alias notebook parameter must be specified\"\n",
        "assert bundle_root != \"\", \"bundle_root notebook parameter must be specified\"\n",
        "\n",
        "# Updating to bundle root\n",
        "import sys \n",
        "sys.path.append(bundle_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a DatabricksFunctionClient and set as default\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unitycatalog.ai.core.base import set_uc_function_client\n",
        "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
        "\n",
        "client = DatabricksFunctionClient()\n",
        "\n",
        "# sets the default uc function client\n",
        "set_uc_function_client(client)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function: execute_python_code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from agent_development.agent.tools.ai_tools import execute_python_code\n",
        "\n",
        "function_info = client.create_python_function(\n",
        "    func=execute_python_code, catalog=uc_catalog, schema=schema, replace=True\n",
        ")\n",
        "python_execution_function_name = function_info.full_name\n",
        "\n",
        "# test execution\n",
        "client.execute_function(python_execution_function_name, {\"code\": \"print(1+1)\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function: ai_function_name_sql\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from agent_development.agent.tools.ai_tools import ask_ai_function\n",
        "\n",
        "ask_ai_function_name = f\"{uc_catalog}.{schema}.ask_ai\"\n",
        "\n",
        "client.create_function(sql_function_body = ask_ai_function.format(ask_ai_function_name = ask_ai_function_name))\n",
        "result = client.execute_function(ask_ai_function_name, {\"question\": \"What is MLflow?\"})\n",
        "result.value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function: summarization_function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from agent_development.agent.tools.ai_tools import summarization_function\n",
        "\n",
        "summarization_function_name = f\"{uc_catalog}.{schema}.summarize\"\n",
        "\n",
        "client.create_function(sql_function_body = summarization_function.format(summarization_function_name = summarization_function_name))\n",
        "# test execution\n",
        "client.execute_function(summarization_function_name, {\"text\": result.value, \"max_words\": int(max_words)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function: translate_function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from agent_development.agent.tools.ai_tools import translate_function\n",
        "\n",
        "translate_function_name = f\"{uc_catalog}.{schema}.translate\"\n",
        "\n",
        "client.create_function(sql_function_body = translate_function.format(translate_function_name = translate_function_name))\n",
        "# test execution\n",
        "client.execute_function(translate_function_name, {\"content\": \"What is MLflow?\", \"language\": \"es\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define UC toolkit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unitycatalog.ai.langchain.toolkit import UCFunctionToolkit\n",
        "\n",
        "# Add tools here\n",
        "toolkit = UCFunctionToolkit(\n",
        "    function_names=[\n",
        "        python_execution_function_name,\n",
        "        # ask_ai_function_name, # commenting out to showcase the retriever\n",
        "        summarization_function_name,\n",
        "        translate_function_name,\n",
        "    ]\n",
        ")\n",
        "\n",
        "uc_tools = toolkit.tools\n",
        "uc_tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import retriever_function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from agent_development.agent.tools.ai_tools import retrieve_function\n",
        "\n",
        "os.environ[\"UC_CATALOG\"] = uc_catalog # Set these before function execution\n",
        "os.environ[\"SCHEMA\"] = schema\n",
        "os.environ[\"VECTOR_SEARCH_INDEX\"] = vector_search_index\n",
        "\n",
        "# retrieve_function(\"what is mlflow?\") # Remove @tool from the retrieve_function in ai_tools.py to test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.langchain.autolog()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the tools in Langgraph\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import Any, Generator, Optional, Sequence, Union, Literal\n",
        "import mlflow\n",
        "from databricks_langchain import (\n",
        "    ChatDatabricks,\n",
        "    UCFunctionToolkit,\n",
        "    VectorSearchRetrieverTool,\n",
        ")\n",
        "from langchain_core.language_models import LanguageModelLike\n",
        "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
        "from langchain_core.tools import BaseTool\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.prebuilt.tool_node import ToolNode\n",
        "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
        "from mlflow.pyfunc import ChatAgent\n",
        "from mlflow.types.agent import (\n",
        "    ChatAgentChunk,\n",
        "    ChatAgentMessage,\n",
        "    ChatAgentResponse,\n",
        "    ChatContext,\n",
        ")\n",
        "\n",
        "tools = uc_tools + [retrieve_function]\n",
        "\n",
        "# Example for Databricks foundation model endpoints\n",
        "model = ChatDatabricks(endpoint=f\"{agent_model_endpoint}\")\n",
        "system_prompt = \"You are a Databricks expert. \"\n",
        "\n",
        "def create_tool_calling_agent(\n",
        "    model: LanguageModelLike, \n",
        "    tools: Union[ToolNode, Sequence[BaseTool]], \n",
        "    system_prompt: Optional[str]=None\n",
        "): \n",
        "    model = model.bind_tools(tools)\n",
        "\n",
        "    # Define the function that determines whether to continue or not\n",
        "    def should_continue(state: ChatAgentState) -> Literal[\"tools\", END]:\n",
        "        messages = state[\"messages\"]\n",
        "        last_message = messages[-1]\n",
        "        # If the LLM makes a tool call, then we route to the \"tools\" node\n",
        "        if last_message.get(\"tool_calls\"):\n",
        "            return \"tools\"\n",
        "        # Otherwise, we stop (reply to the user)\n",
        "        return END\n",
        "\n",
        "    preprocessor = RunnableLambda(\n",
        "        lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "        + state[\"messages\"]\n",
        "    )\n",
        "    model_runnable = preprocessor | model\n",
        "\n",
        "    # Define the function that calls the model\n",
        "    def call_model(state: ChatAgentState, config: RunnableConfig):\n",
        "        # Loop to make sure the tool call is executed correctly\n",
        "        failing = True\n",
        "        retry = 10\n",
        "        while failing and retry>=0: \n",
        "            try: \n",
        "                response = model_runnable.invoke(state, config)\n",
        "                failing = False\n",
        "            except: \n",
        "                retry -= 1\n",
        "        # We return a list, because this will get added to the existing list\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    # Define a new graph\n",
        "    workflow = StateGraph(ChatAgentState)\n",
        "\n",
        "    # Define the two nodes we will cycle between\n",
        "    tool_node = ChatAgentToolNode(tools)\n",
        "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
        "    workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "    # Set the entrypoint as `agent`\n",
        "    # This means that this node is the first one called\n",
        "    workflow.add_edge(START, \"agent\")\n",
        "\n",
        "    # We now add a conditional edge\n",
        "    workflow.add_conditional_edges(\n",
        "        # First, we define the start node. We use `agent`.\n",
        "        # This means these are the edges taken after the `agent` node is called.\n",
        "        \"agent\",\n",
        "        # Next, we pass in the function that will determine which node is called next.\n",
        "        should_continue,\n",
        "    )\n",
        "\n",
        "    # We now add a normal edge from `tools` to `agent`.\n",
        "    # This means that after `tools` is called, `agent` node is called next.\n",
        "    workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "    return workflow.compile()\n",
        "\n",
        "app = create_tool_calling_agent(model, tools, system_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_state = app.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Retrieve the documentation for MLflow. Keep the response concise and reply in Spanish. Try using as many tools as possible\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        ")\n",
        "response = final_state[\"messages\"][-1].get('content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_state = app.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Remember to always try using tools. Can you convert the following explanation to English? {response}\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        ")\n",
        "final_state[\"messages\"][-1].get('content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "final_state = app.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}]},\n",
        ")\n",
        "final_state[\"messages\"][-1].get('content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Log the model using MLflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%writefile app.py\n",
        "from typing import Any, Generator, Optional, Sequence, Union, Literal\n",
        "import mlflow\n",
        "from databricks_langchain import (\n",
        "    ChatDatabricks,\n",
        "    UCFunctionToolkit,\n",
        "    VectorSearchRetrieverTool,\n",
        ")\n",
        "from langchain_core.language_models import LanguageModelLike\n",
        "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
        "from langchain_core.tools import BaseTool, tool\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.prebuilt.tool_node import ToolNode\n",
        "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
        "from mlflow.pyfunc import ChatAgent\n",
        "from mlflow.types.agent import (\n",
        "    ChatAgentChunk,\n",
        "    ChatAgentMessage,\n",
        "    ChatAgentResponse,\n",
        "    ChatContext,\n",
        ")\n",
        "\n",
        "## Load the agent's configuration\n",
        "model_config = mlflow.models.ModelConfig(development_config=\"config.yaml\")\n",
        "\n",
        "uc_catalog = model_config.get(\"uc_catalog\")\n",
        "schema = model_config.get(\"schema\")\n",
        "vector_search_index = model_config.get(\"vector_search_index\")\n",
        "\n",
        "python_execution_function_name = f\"{uc_catalog}.{schema}.execute_python_code\"\n",
        "ask_ai_function_name = f\"{uc_catalog}.{schema}.ask_ai\"\n",
        "summarization_function_name = f\"{uc_catalog}.{schema}.summarize\"\n",
        "translate_function_name = f\"{uc_catalog}.{schema}.translate\"\n",
        "\n",
        "@tool\n",
        "def retrieve_function(query: str) -> str:\n",
        "    \"\"\"Retrieve from Databricks Vector Search using the query.\"\"\"\n",
        "\n",
        "    index = f\"{uc_catalog}.{schema}.{vector_search_index}\"\n",
        "\n",
        "    vs_tool = VectorSearchRetrieverTool(\n",
        "        index_name=index,\n",
        "        tool_name=\"vector_search_retriever\",\n",
        "        tool_description=\"Retrieves information from Databricks Vector Search.\",\n",
        "        embedding_model_name=\"databricks-bge-large-en\", \n",
        "        num_results=1, \n",
        "        columns=[\"url\", \"content\"],\n",
        "        query_type=\"ANN\" \n",
        "    )\n",
        "\n",
        "    response = vs_tool.invoke(query)\n",
        "    return f\"{response[0].metadata['url']}  \\n{response[0].page_content}\"\n",
        "  \n",
        "toolkit = UCFunctionToolkit(\n",
        "  function_names=[\n",
        "    python_execution_function_name,\n",
        "    # ask_ai_function_name, # commenting out to showcase retriever\n",
        "    summarization_function_name,\n",
        "    translate_function_name,\n",
        "    ]\n",
        ")\n",
        "uc_tools = toolkit.tools\n",
        "tools = uc_tools + [retrieve_function]\n",
        "\n",
        "# Example for Databricks foundation model endpoints\n",
        "model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\")\n",
        "system_prompt = \"You are a Databricks expert. \"\n",
        "\n",
        "def create_tool_calling_agent(\n",
        "    model: LanguageModelLike, \n",
        "    tools: Union[ToolNode, Sequence[BaseTool]], \n",
        "    system_prompt: Optional[str]=None\n",
        "): \n",
        "    model = model.bind_tools(tools)\n",
        "\n",
        "    def should_continue(state: ChatAgentState) -> Literal[\"tools\", END]:\n",
        "        messages = state[\"messages\"]\n",
        "        last_message = messages[-1]\n",
        "        if last_message.get(\"tool_calls\"):\n",
        "            return \"tools\"\n",
        "        return END\n",
        "\n",
        "    preprocessor = RunnableLambda(lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"])\n",
        "    model_runnable = preprocessor | model\n",
        "\n",
        "    def call_model(state: ChatAgentState, config: RunnableConfig):\n",
        "        failing = True\n",
        "        retry = 10\n",
        "        while failing and retry>=0: \n",
        "            try: \n",
        "                response = model_runnable.invoke(state, config)\n",
        "                failing = False\n",
        "            except: \n",
        "                retry -= 1\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    workflow = StateGraph(ChatAgentState)\n",
        "\n",
        "    tool_node = ChatAgentToolNode(tools)\n",
        "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
        "    workflow.add_node(\"tools\", tool_node)\n",
        "    workflow.add_edge(START, \"agent\")\n",
        "    workflow.add_conditional_edges(\"agent\", should_continue)\n",
        "    workflow.add_edge(\"tools\", \"agent\")\n",
        "    return workflow.compile()\n",
        "\n",
        "class LangGraphChatAgent(ChatAgent):\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        messages: list[ChatAgentMessage],\n",
        "        context: Optional[ChatContext] = None,\n",
        "        custom_inputs: Optional[dict[str, Any]] = None,\n",
        "    ) -> ChatAgentResponse:\n",
        "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
        "\n",
        "        messages = []\n",
        "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
        "            for node_data in event.values():\n",
        "                messages.extend(\n",
        "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
        "                )\n",
        "        return ChatAgentResponse(messages=messages)\n",
        "\n",
        "    def predict_stream(\n",
        "        self,\n",
        "        messages: list[ChatAgentMessage],\n",
        "        context: Optional[ChatContext] = None,\n",
        "        custom_inputs: Optional[dict[str, Any]] = None,\n",
        "    ) -> Generator[ChatAgentChunk, None, None]:\n",
        "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
        "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
        "            for node_data in event.values():\n",
        "                yield from (\n",
        "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
        "                )\n",
        "\n",
        "# Create the agent object, and specify it as the agent object to use when\n",
        "# loading the agent back for inference via mlflow.models.set_model()\n",
        "mlflow.langchain.autolog()\n",
        "agent = create_tool_calling_agent(model, tools, system_prompt)\n",
        "AGENT = LangGraphChatAgent(agent)\n",
        "mlflow.models.set_model(AGENT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import yaml\n",
        "\n",
        "agent_config = {\n",
        "    \"uc_catalog\": uc_catalog,\n",
        "    \"schema\": schema,\n",
        "    \"vector_search_endpoint\": vector_search_endpoint,\n",
        "    \"vector_search_index\": f\"{uc_catalog}.{schema}.{vector_search_index}\",\n",
        "}\n",
        "\n",
        "with open(\"config.yaml\", \"w\") as f:\n",
        "    yaml.dump(agent_config, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mlflow\n",
        "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint, DatabricksVectorSearchIndex\n",
        "from pkg_resources import get_distribution\n",
        "from pyspark.sql.functions import session_user\n",
        "\n",
        "user_name = spark.range(1).select(session_user()).collect()[0][0]\n",
        "\n",
        "mlflow.set_experiment(experiment)\n",
        "\n",
        "resources = [\n",
        "    DatabricksServingEndpoint(endpoint_name=agent_model_endpoint), \n",
        "    DatabricksFunction(f\"{uc_catalog}.{schema}.execute_python_code\"), \n",
        "    DatabricksFunction(f\"{uc_catalog}.{schema}.ask_ai\"), \n",
        "    DatabricksFunction(f\"{uc_catalog}.{schema}.summarize\"), \n",
        "    DatabricksFunction(f\"{uc_catalog}.{schema}.translate\"), \n",
        "    DatabricksVectorSearchIndex(index_name=f\"{uc_catalog}.{schema}.{vector_search_index}\")\n",
        "]\n",
        "\n",
        "with mlflow.start_run():\n",
        "    model_info = mlflow.pyfunc.log_model(\n",
        "        python_model=\"../notebooks/app.py\", # Pass the path to the saved model file\n",
        "        model_config=\"../notebooks/config.yaml\", # Agent configuration \n",
        "        name=\"model\",\n",
        "        resources=resources, \n",
        "        pip_requirements=[\n",
        "            f\"databricks-connect=={get_distribution('databricks-connect').version}\",\n",
        "            f\"unitycatalog-langchain[databricks]=={get_distribution('unitycatalog-langchain[databricks]').version}\",\n",
        "            f\"databricks-vectorsearch=={get_distribution('databricks-vectorsearch').version}\",\n",
        "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
        "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
        "            f\"mlflow=={get_distribution('mlflow').version}\",\n",
        "        ],\n",
        "        registered_model_name=f\"{uc_catalog}.{schema}.{registered_model}\"  # Replace with your own model name\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set Alias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mlflow import MlflowClient\n",
        "\n",
        "# Initialize MLflow client\n",
        "client = MlflowClient()\n",
        "\n",
        "# Set an alias for new version of the registered model to retrieve it for model serving\n",
        "client.set_registered_model_alias(f\"{uc_catalog}.{schema}.{registered_model}\", model_alias, model_info.registered_model_version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate the model locally prior to serving\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from mlflow.models import convert_input_example_to_serving_input, validate_serving_input\n",
        "\n",
        "serving_input = convert_input_example_to_serving_input(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is MLflow?\"}]}\n",
        ")\n",
        "validate_serving_input(model_info.model_uri, serving_input=serving_input)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}