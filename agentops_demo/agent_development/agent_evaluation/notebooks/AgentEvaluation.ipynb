{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "#%pip install -r ../../agent_requirements.txt\n",
        "#dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
        "# To disable autoreload; run %autoreload 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##################################################################################\n",
        "# Agent Evaluation\n",
        "# \n",
        "# Notebook that downloads an evaluation dataset and evaluates the model using\n",
        "# llm-as-a-judge with the Databricks agent framework.\n",
        "#\n",
        "# Parameters:\n",
        "# * uc_catalog (required)           - Name of the Unity Catalog \n",
        "# * schema (required)               - Name of the schema inside Unity Catalog \n",
        "# * eval_table (required)           - Name of the table containing the evaluation dataset\n",
        "# * experiment (required)           - Name of the experiment to register the run under\n",
        "# * registered_model (required)     - Name of the model registered in mlflow\n",
        "# * model_alias (required)          - Model alias to deploy\n",
        "# * bundle_root (required)          - Root of the bundle\n",
        "#\n",
        "# Widgets:\n",
        "# * Unity Catalog: Text widget to input the name of the Unity Catalog\n",
        "# * Schema: Text widget to input the name of the database inside the Unity Catalog\n",
        "# * Evaluation Table: Text widget to input the name of the table containing the evaluation dataset\n",
        "# * Experiment: Text widget to input the name of the experiment to register the run under\n",
        "# * Registered model name: Text widget to input the name of the model to register in mlflow\n",
        "# * Model Alias: Text widget to input the model alias to deploy\n",
        "# * Bundle root: Text widget to input the root of the bundle\n",
        "#\n",
        "# Usage:\n",
        "# 1. Set the appropriate values for the widgets.\n",
        "# 2. Run to evaluate your agent.\n",
        "#\n",
        "##################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of input args needed to run the notebook as a job.\n",
        "# Provide them via DB widgets or notebook arguments.\n",
        "\n",
        "# A Unity Catalog containing the model\n",
        "dbutils.widgets.text(\n",
        "    \"uc_catalog\",\n",
        "    \"agentops_stacks_dev\",\n",
        "    label=\"Unity Catalog\",\n",
        ")\n",
        "# Name of schema\n",
        "dbutils.widgets.text(\n",
        "    \"schema\",\n",
        "    \"agentops\",\n",
        "    label=\"Schema\",\n",
        ")\n",
        "# Name of evaluation table\n",
        "dbutils.widgets.text(\n",
        "    \"eval_table\",\n",
        "    \"databricks_documentation_eval\",\n",
        "    label=\"Evaluation dataset\",\n",
        ")\n",
        "# Name of experiment to register under in mlflow\n",
        "dbutils.widgets.text(\n",
        "    \"experiment\",\n",
        "    \"agent_function_chatbot_dev\",\n",
        "    label=\"Experiment name\",\n",
        ")\n",
        "# Name of model registered in mlflow\n",
        "dbutils.widgets.text(\n",
        "    \"registered_model\",\n",
        "    \"agent_function_chatbot\",\n",
        "    label=\"Registered model name\",\n",
        ")\n",
        "# Model alias\n",
        "dbutils.widgets.text(\n",
        "    \"model_alias\",\n",
        "    \"agent_latest\",\n",
        "    label=\"Model Alias\",\n",
        ")\n",
        "# Bundle root\n",
        "dbutils.widgets.text(\n",
        "    \"bundle_root\",\n",
        "    \"/\",\n",
        "    label=\"Root of bundle\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uc_catalog = dbutils.widgets.get(\"uc_catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "eval_table = dbutils.widgets.get(\"eval_table\")\n",
        "experiment = dbutils.widgets.get(\"experiment\")\n",
        "registered_model = dbutils.widgets.get(\"registered_model\")\n",
        "model_alias = dbutils.widgets.get(\"model_alias\")\n",
        "bundle_root = dbutils.widgets.get(\"bundle_root\")\n",
        "\n",
        "assert uc_catalog != \"\", \"uc_catalog notebook parameter must be specified\"\n",
        "assert schema != \"\", \"schema notebook parameter must be specified\"\n",
        "assert eval_table != \"\", \"eval_table notebook parameter must be specified\"\n",
        "assert experiment != \"\", \"experiment notebook parameter must be specified\"\n",
        "assert registered_model != \"\", \"registered_model notebook parameter must be specified\"\n",
        "assert model_alias != \"\", \"model_alias notebook parameter must be specified\"\n",
        "assert bundle_root != \"\", \"bundle_root notebook parameter must be specified\"\n",
        "\n",
        "# Updating to bundle root\n",
        "import sys \n",
        "sys.path.append(bundle_root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Evaluation Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mlflow.genai.datasets\n",
        "\n",
        "try: \n",
        "    eval_dataset = mlflow.genai.datasets.create_dataset(\n",
        "        uc_table_name=f\"{uc_catalog}.{schema}.{eval_table}\",\n",
        "    )\n",
        "except: \n",
        "    # Eval table already exists \n",
        "    eval_dataset = mlflow.genai.datasets.get_dataset(\n",
        "        uc_table_name=f\"{uc_catalog}.{schema}.{eval_table}\",\n",
        "    )\n",
        "\n",
        "print(f\"Evaluation dataset: {uc_catalog}.{schema}.{eval_table}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Reference Documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from agent_development.agent_evaluation.evaluation.evaluation import get_reference_documentation\n",
        "\n",
        "reference_docs = get_reference_documentation(uc_catalog, schema, eval_table, spark)\n",
        "\n",
        "display(reference_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge Reference Docs to Eval Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eval_dataset.merge_records(reference_docs.limit(100))\n",
        "\n",
        "# Preview the dataset\n",
        "df = eval_dataset.to_df()\n",
        "print(f\"\\nDataset preview:\")\n",
        "print(f\"Total records: {len(df)}\")\n",
        "print(\"\\nSample record:\")\n",
        "sample = df.iloc[0]\n",
        "print(f\"Inputs: {sample['inputs']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import mlflow \n",
        "from mlflow.genai.scorers import scorer\n",
        "from mlflow.genai.scorers import RetrievalRelevance, RetrievalGroundedness\n",
        "from pyspark.sql.functions import session_user\n",
        "import re\n",
        "\n",
        "# Workaround for serverless compatibility\n",
        "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
        "\n",
        "model = mlflow.pyfunc.load_model(f\"models:/{uc_catalog}.{schema}.{registered_model}@{model_alias}\")\n",
        "def evaluate_model(question):\n",
        "    return model.predict({\"messages\": [{\"role\": \"user\", \"content\": question}]})\n",
        "\n",
        "user_name = spark.range(1).select(session_user()).collect()[0][0]\n",
        "\n",
        "mlflow.set_experiment(experiment)\n",
        "\n",
        "\n",
        "with mlflow.start_run():\n",
        "    # Evaluate the logged model\n",
        "    eval_results = mlflow.genai.evaluate(\n",
        "        data=eval_dataset,\n",
        "        predict_fn=evaluate_model,\n",
        "        scorers=[RetrievalRelevance(), RetrievalGroundedness()],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}