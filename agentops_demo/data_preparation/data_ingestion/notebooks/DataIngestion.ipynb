{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "#%pip install -r ../../data_prep_requirements.txt\n",
        "#dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
        "# To disable autoreload; run %autoreload 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "################################################################################### \n",
        "# Data Ingestion Pipeline\n",
        "#\n",
        "# This pipeline is designed to process raw documentation data from a specified data source URL. \n",
        "# The data is stored in a Unity Catalog within a specified database for later processing.\n",
        "#\n",
        "# Parameters:\n",
        "# * uc_catalog (required)                     - Name of the Unity Catalog containing the input data\n",
        "# * schema (required)                         - Name of the schema inside the Unity Catalog\n",
        "# * raw_data_table (required)                 - Name of the raw data table inside the database of the Unity Catalog\n",
        "# * data_source_url (required)                - URL of the data source. Default is \"https://docs.databricks.com/en/doc-sitemap.xml\"\n",
        "# * bundle_root (required)                    - Root of the bundle\n",
        "#\n",
        "# Widgets:\n",
        "# * Unity Catalog: Text widget to input the name of the Unity Catalog\n",
        "# * Schema: Text widget to input the name of the database inside the Unity Catalog\n",
        "# * Raw data table: Text widget to input the name of the raw data table inside the database of the Unity Catalog\n",
        "# * Data Source URL: Text widget to input the URL of the data source\n",
        "# * Root of bundle: Text widget to input the root of the bundle\n",
        "#\n",
        "# Usage:\n",
        "# 1. Set the appropriate values for the widgets.\n",
        "# 2. Run the pipeline to collect and store the raw documentation data.\n",
        "#\n",
        "##################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Widget creation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of input args needed to run this notebook as a job\n",
        "# Provide them via DB widgets or notebook arguments in your DAB resources\n",
        "\n",
        "# A Unity Catalog containing the input data\n",
        "dbutils.widgets.text(\n",
        "    \"uc_catalog\",\n",
        "    \"agentops_stacks_dev\",\n",
        "    label=\"Unity Catalog\",\n",
        ")\n",
        "# Name of schema\n",
        "dbutils.widgets.text(\n",
        "    \"schema\",\n",
        "    \"agentops\",\n",
        "    label=\"Schema\",\n",
        ")\n",
        "# Name of raw data table\n",
        "dbutils.widgets.text(\n",
        "    \"raw_data_table\",\n",
        "    \"raw_documentation\",\n",
        "    label=\"Raw data table\",\n",
        ")\n",
        "\n",
        "# Data source url\n",
        "dbutils.widgets.text(\n",
        "    \"data_source_url\",\n",
        "    \"https://docs.databricks.com/en/doc-sitemap.xml\",\n",
        "    label=\"Data Source URL\",\n",
        ")\n",
        "\n",
        "# Bundle root\n",
        "dbutils.widgets.text(\n",
        "    \"bundle_root\",\n",
        "    \"/\",\n",
        "    label=\"Root of bundle\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define input and output variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uc_catalog = dbutils.widgets.get(\"uc_catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "raw_data_table = dbutils.widgets.get(\"raw_data_table\")\n",
        "data_source_url = dbutils.widgets.get(\"data_source_url\")\n",
        "bundle_root = dbutils.widgets.get(\"bundle_root\")\n",
        "\n",
        "assert uc_catalog != \"\", \"uc_catalog notebook parameter must be specified\"\n",
        "assert schema != \"\", \"schema notebook parameter must be specified\"\n",
        "assert raw_data_table != \"\", \"raw_data_table notebook parameter must be specified\"\n",
        "assert data_source_url != \"\", \"data_source_url notebook parameter must be specified\"\n",
        "assert bundle_root != \"\", \"bundle_root notebook parameter must be specified\"\n",
        "\n",
        "# Updating to bundle root\n",
        "import sys \n",
        "\n",
        "root = dbutils.widgets.get(\"bundle_root\")\n",
        "sys.path.append(root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the catalog and database specified in the notebook parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spark.sql(f\"\"\"CREATE SCHEMA IF NOT EXISTS `{uc_catalog}`.`{schema}`\"\"\")\n",
        "\n",
        "spark.sql(f\"\"\"USE `{uc_catalog}`.`{schema}`\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Download and store data to UC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from data_preparation.data_ingestion.ingestion.fetch_data import fetch_data_from_url\n",
        "\n",
        "if not spark.catalog.tableExists(f\"{raw_data_table}\") or spark.table(f\"{raw_data_table}\").isEmpty():\n",
        "    # Download the data to a DataFrame \n",
        "    doc_articles = fetch_data_from_url(spark, data_source_url)\n",
        "\n",
        "    #Save them as to unity catalog\n",
        "    doc_articles.write.mode('overwrite').saveAsTable(f\"{raw_data_table}\")\n",
        "\n",
        "    doc_articles.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dbutils.notebook.exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}