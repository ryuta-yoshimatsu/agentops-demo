{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Databricks notebook source\n",
        "#%pip install -r ../../data_prep_requirements.txt\n",
        "#dbutils.library.restartPython()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
        "# To disable autoreload; run %autoreload 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "################################################################################### \n",
        "# Data Preprocessing Pipeline\n",
        "#\n",
        "# This notebook shows an example of a Data Preprocessing pipeline using Unity Catalog.\n",
        "# It is configured and can be executed as the tasks in the PreprocessRawData workflow defined under\n",
        "# ``agentops_demo/resources/data-preprocessing-workflow-resource.yml``\n",
        "#\n",
        "# Parameters:\n",
        "# * uc_catalog (required)                     - Name of the Unity Catalog \n",
        "# * schema (required)                         - Name of the schema inside Unity Catalog \n",
        "# * raw_data_table (required)                 - Name of the raw data table inside UC database\n",
        "# * preprocessed_data_table (required)        - Name of the preprocessed data table inside UC database\n",
        "# * hf_tokenizer_model (optional)             - Name of the HuggingFace tokenizer model name\n",
        "# * max_chunk_size (optional)                 - Maximum chunk size\n",
        "# * min_chunk_size (optional)                 - Minimum chunk size\n",
        "# * chunk_overlap (optional)                  - Overlap between chunks \n",
        "# * bundle_root (required)                    - Root of the bundle\n",
        "#\n",
        "# Widgets:\n",
        "# * Unity Catalog: Text widget to input the name of the Unity Catalog\n",
        "# * schema: Text widget to input the name of the database inside the Unity Catalog\n",
        "# * Raw data table: Text widget to input the name of the raw data table inside the database of the Unity Catalog\n",
        "# * Preprocessed data table: Text widget to input the name of the preprocessed data table inside the database of the Unity Catalog\n",
        "# * HuggingFace tokenizer model: Text widget to input the name of the hugging face tokenizer model to import\n",
        "# * Maximum chunk size: Maximum characters chunks will be split into\n",
        "# * Minimum chunk size: minimum characters chunks will be split into\n",
        "# * Chunk overlap: Overlap between chunks\n",
        "# * Root of bundle: Text widget to input the root of the bundle\n",
        "#\n",
        "# Usage:\n",
        "# 1. Set the appropriate values for the widgets.\n",
        "# 2. Run the pipeline to chunk the raw data and store in Unity Catalog.\n",
        "#\n",
        "##################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of input args needed to run this notebook as a job.\n",
        "# Provide them via DB widgets or notebook arguments.\n",
        "\n",
        "# A Unity Catalog containing the input data\n",
        "dbutils.widgets.text(\n",
        "    \"uc_catalog\",\n",
        "    \"agentops_stacks_dev\",\n",
        "    label=\"Unity Catalog\",\n",
        ")\n",
        "# Name of schema\n",
        "dbutils.widgets.text(\n",
        "    \"schema\",\n",
        "    \"agentops\",\n",
        "    label=\"Schema\",\n",
        ")\n",
        "# Name of input table\n",
        "dbutils.widgets.text(\n",
        "    \"raw_data_table\",\n",
        "    \"raw_documentation\",\n",
        "    label=\"Raw data table\",\n",
        ")\n",
        "# Name of output table\n",
        "dbutils.widgets.text(\n",
        "    \"preprocessed_data_table\",\n",
        "    \"databricks_documentation\",\n",
        "    label=\"Preprocessed data table\",\n",
        ")\n",
        "# Name of huggingface tokenizer model\n",
        "dbutils.widgets.text(\n",
        "    \"hf_tokenizer_model\",\n",
        "    \"openai-community/openai-gpt\",\n",
        "    label=\"HuggingFace tokenizer model\",\n",
        ")\n",
        "# Maximum chunk size\n",
        "dbutils.widgets.text(\"max_chunk_size\", \"500\", label=\"Maximum chunk size\")\n",
        "# Minimum chunk size\n",
        "dbutils.widgets.text(\"min_chunk_size\", \"20\", label=\"Minimum chunk size\")\n",
        "# Chunk overlap\n",
        "dbutils.widgets.text(\"chunk_overlap\", \"50\", label=\"Chunk overlap\")\n",
        "\n",
        "# Bundle root\n",
        "dbutils.widgets.text(\n",
        "    \"bundle_root\",\n",
        "    \"/\",\n",
        "    label=\"Root of bundle\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define input and output variables\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "uc_catalog = dbutils.widgets.get(\"uc_catalog\")\n",
        "schema = dbutils.widgets.get(\"schema\")\n",
        "raw_data_table = dbutils.widgets.get(\"raw_data_table\")\n",
        "preprocessed_data_table = dbutils.widgets.get(\"preprocessed_data_table\")\n",
        "hf_tokenizer_model = dbutils.widgets.get(\"hf_tokenizer_model\")\n",
        "max_chunk_size = int(dbutils.widgets.get(\"max_chunk_size\"))\n",
        "min_chunk_size = int(dbutils.widgets.get(\"min_chunk_size\"))\n",
        "chunk_overlap = int(dbutils.widgets.get(\"chunk_overlap\"))\n",
        "bundle_root = dbutils.widgets.get(\"bundle_root\")\n",
        "\n",
        "assert uc_catalog != \"\", \"uc_catalog notebook parameter must be specified\"\n",
        "assert schema != \"\", \"schema notebook parameter must be specified\"\n",
        "assert raw_data_table != \"\", \"raw_data_table notebook parameter must be specified\"\n",
        "assert preprocessed_data_table != \"\", \"preprocessed_data_table notebook parameter must be specified\"\n",
        "assert hf_tokenizer_model != \"\", \"hf_tokenizer_model notebook parameter must be specified\"\n",
        "assert max_chunk_size != \"\", \"max_chunk_size notebook parameter must be specified\"\n",
        "assert min_chunk_size != \"\", \"min_chunk_size notebook parameter must be specified\"\n",
        "assert chunk_overlap != \"\", \"chunk_overlap notebook parameter must be specified\"\n",
        "assert bundle_root != \"\", \"bundle_root notebook parameter must be specified\"\n",
        "\n",
        "# Updating to bundle root\n",
        "import sys \n",
        "\n",
        "root = dbutils.widgets.get(\"bundle_root\")\n",
        "sys.path.append(root)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download tokenizer model to UC volume\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "volume_folder =  f\"/Volumes/{uc_catalog}/{schema}/volume_databricks_documentation\"\n",
        "\n",
        "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {uc_catalog}.{schema}.volume_databricks_documentation\")\n",
        "\n",
        "# Initialize tokenizer once\n",
        "tokenizer = AutoTokenizer.from_pretrained(hf_tokenizer_model, cache_dir=f'{volume_folder}/hg_cache')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use the catalog and database specified in the notebook parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "spark.sql(f\"\"\"USE `{uc_catalog}`.`{schema}`\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create output preprocessed data table\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if not spark.catalog.tableExists(f\"{preprocessed_data_table}\") or spark.table(f\"{preprocessed_data_table}\").isEmpty():\n",
        "  spark.sql(f\"\"\"\n",
        "  CREATE TABLE IF NOT EXISTS {preprocessed_data_table} (\n",
        "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
        "    url STRING,\n",
        "    content STRING\n",
        "  )\n",
        "  TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true')\n",
        "  \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a user-defined function (UDF) to chunk all our documents with spark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from functools import partial\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from data_preparation.data_preprocessing.preprocessing.create_chunk import split_html_on_p\n",
        "\n",
        "@pandas_udf(\"array<string>\")\n",
        "def parse_and_split(\n",
        "    docs: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"Parse and split html content into chunks.\n",
        "\n",
        "    :param docs: Input documents\n",
        "    :return: List of chunked text for each input document\n",
        "    \"\"\"\n",
        "    \n",
        "    return docs.apply(lambda html: split_html_on_p(\n",
        "        html,\n",
        "        tokenizer=tokenizer,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        min_chunk_size=min_chunk_size,\n",
        "        max_chunk_size=max_chunk_size\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform data preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "(spark.table(raw_data_table)\n",
        "      .filter('text is not null')\n",
        "      .withColumn('content', F.explode(parse_and_split('text')))\n",
        "      .drop(\"text\")\n",
        "      .write.mode('overwrite').saveAsTable(preprocessed_data_table))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dbutils.notebook.exit(0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}